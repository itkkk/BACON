{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some \"service function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_stats(train_x, train_y, test_x, test_y):\n",
    "    \"\"\"\n",
    "    Prints some statistics about the dataset.\n",
    "    \n",
    "    Parameter:\n",
    "    - train_x : numpy array -> the part of the training set containing the features\n",
    "    - train_y : numpy array -> the part of the training set containing the target value\n",
    "    - test_x : numpy array -> the part of the test set containing the features\n",
    "    - test_y : numpy array -> the part of the test set containing the target value\n",
    "    \"\"\" \n",
    "    m_train = train_x.shape[0]\n",
    "    num_px = train_x.shape[1]\n",
    "    m_test = test_x.shape[0]\n",
    "\n",
    "    print (\"Number of training examples: \" + str(m_train))\n",
    "    print (\"Number of testing examples: \" + str(m_test))\n",
    "    print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "    print (\"train x shape: \" + str(train_x.shape))\n",
    "    print (\"train y shape: \" + str(train_y.shape))\n",
    "    print (\"test x shape: \" + str(test_x.shape))\n",
    "    print (\"test y shape: \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_examples(x, y, classes):\n",
    "    \"\"\"\n",
    "    Prints 20 example images from the dataset\n",
    "    \n",
    "    Parameter:\n",
    "    - x : numpy array -> the part of the dataset containing the features (trivially the value of each pixel of the image) \n",
    "    - y : numpy array -> the part of the dataset containing the target value (the class assigned to each image)\n",
    "    - classes : numpy array -> an array containing the classes name\n",
    "    \"\"\"\n",
    "    plt.rcParams['figure.figsize'] = (40.0, 40.0) # set default size of plots\n",
    "    for i in range(20):\n",
    "        plt.subplot(4, 5, i + 1)\n",
    "        plt.imshow(x[i], interpolation='nearest')\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Class: \" + classes[y[0,i]].decode(\"utf-8\"), fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cat_dataset(print_stats = False, print_examples = False):\n",
    "    \"\"\"\n",
    "    Loads and preprocess the h5 dataset catvnoncat.\n",
    "    \n",
    "    Parameters:\n",
    "    - printstat : Boolean -> indicates whether to print or not the main statistics about the dataset \n",
    "    - print_examples : Boolean -> indicates weather to show or not some examples from the dataset\n",
    "    \n",
    "    Returns:\n",
    "    - train_x: a 12288*209 numpy array containing the flattened training images on its column\n",
    "    - train_y: a 1*209 numpy array containing the target value for the train\n",
    "    - test_x: a 12288*50 numpy array containing the flattened test images on its column\n",
    "    - test_y: a 1*50 numpy array containing the target value for the test\n",
    "    - classes: a numpy array containing the two classes name (\"cat\", \"non-cat\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    N.B.\n",
    "    The dataset is physically splitted into two h5 file: train_catvnoncat.h5 and test_catvnoncat.h5.\n",
    "    Each h5 file contains 3 subsection: \n",
    "     - train/test_set_x with the features;\n",
    "     - train/test_y with the target value;\n",
    "     - list_classes containing the two classes name (\"cat\", \"non-cat\")\n",
    "    \"\"\" \n",
    "    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # train set features\n",
    "    train_y = np.array(train_dataset[\"train_set_y\"][:]) # train set labels\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # test set features\n",
    "    test_y = np.array(test_dataset[\"test_set_y\"][:]) # test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "        \n",
    "    train_y = train_y.reshape((1, train_y.shape[0])) # since train set labels have shape (209,) we reshape the array in order to eliminate teh rank-1 array and have all the labels in separate columns\n",
    "    test_y = test_y.reshape((1, test_y.shape[0])) # since test set labels have shape (50,) we reshape the array in order to eliminate teh rank-1 array and have all the labels in separate columns\n",
    "    \n",
    "    if print_stats:\n",
    "        print_dataset_stats(train_x_orig, train_y, test_x_orig, test_y)\n",
    "        \n",
    "    if print_examples:\n",
    "        print_dataset_examples(train_x_orig, train_y, classes)\n",
    "        \n",
    "    #preprocessing part\n",
    "    # Reshape the training and test examples \n",
    "    train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "    test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "    # Standardize data to have feature values between 0 and 1.\n",
    "    train_x = train_x_flatten/255.\n",
    "    test_x = test_x_flatten/255.\n",
    "    \n",
    "    if print_stats:\n",
    "        print (\"train x shape after preprocessing: \" + str(train_x.shape))\n",
    "        print (\"test x shape after preprocessing: \" + str(test_x.shape))\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_indian_dataset(print_stats = False):\n",
    "    \"\"\"\n",
    "    Loads the pima-indians-diabetes dataset, which contains informations used to diagnostically predict whether or not a \n",
    "    patient has diabetes, based on certain diagnostic measurements.\n",
    "    \n",
    "    Parameters:\n",
    "    - printstat : Boolean -> indicates whether to print or not the main statistics about the dataset \n",
    "Ã¬    \n",
    "    Returns:\n",
    "    - train_x_orig: a 8*609 numpy array containing the flattened training images on its column\n",
    "    - train_y: a 1*609 numpy array containing the target value for the train\n",
    "    - test_x_orig: a 8*159 numpy array containing the flattened test images on its column\n",
    "    - test_y: a 1*159 numpy array containing the target value for the test\n",
    "    \n",
    "    \"\"\"\n",
    "    train_dataset = np.loadtxt('datasets/pima-indians-diabetes.txt', delimiter = \",\" ) \n",
    "    train_x_orig = train_dataset[:,:-1].T\n",
    "    train_y = train_dataset[:,-1]\n",
    "\n",
    "\n",
    "    test_dataset = np.loadtxt('datasets/pima-indians-diabetes-test.txt', delimiter = \",\" ) \n",
    "    test_x_orig = test_dataset[:,:-1].T\n",
    "    test_y = test_dataset[:,-1]\n",
    "\n",
    "\n",
    "    train_y = train_y.reshape((1, train_y.shape[0])) \n",
    "    test_y = test_y.reshape((1, test_y.shape[0]))\n",
    "\n",
    "    if print_stats:\n",
    "        print(\"train x shape: \" + str(np.shape(train_x_orig)))\n",
    "        print(\"train y shape: \" + str(np.shape(train_y)))\n",
    "        print(\"test x shape: \" + str(np.shape(test_x_orig)))\n",
    "        print(\"test y shape: \" + str(np.shape(test_y)))\n",
    "\n",
    "    return train_x_orig, train_y, test_x_orig, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(p,y):\n",
    "    \"\"\"\n",
    "    Return the accuracy of a model\n",
    "    \n",
    "    Parameters:\n",
    "    - p : numpy array -> the model prediction\n",
    "    - y : numpy array -> the correct label for the predictions\n",
    "    \"\"\"\n",
    "    m = y.shape[1]\n",
    "    return np.sum((p == y)/m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The activation functions with their derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Parameters:\n",
    "    Z : a numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A : a numpy array containing the output of sigmoid(z), same shape as Z\n",
    "    cache : returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Parameters:\n",
    "    dA : post-activation gradient, of any shape\n",
    "    cache : 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ : Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z : Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A : a numpy array containing the output of relu(z), same shape as Z\n",
    "    cache : returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    cache = Z \n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA : post-activation gradient, of any shape\n",
    "    cache : 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ : Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The neural network class\n",
    "This class is useful to define a neural network for a binary classification problem. The class definines the basic methods used in machine learning and it exploits numpy vectorization in order to avoid unnecessary for loops. \n",
    "\n",
    "The fit method performs the classic steps useful to train the model:\n",
    " - <b>Forward pass</b>: in which the weights for each layer are updated. <br>\n",
    "   It starts from an input vector A, which represents the output of l-1 layer (we assume that $A^{[0]} = X$, which is the input given to the first layer of the network). \n",
    "   Then it computes Z as: $Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$.\n",
    "   Finally the activation for each node of the layer is computed as $A^{[l]} = g(Z^{[l]})$. Depending on whether the layer is the last or not, $g$ is the sigmoid function ($\\sigma(Z) = \\frac{1}{ 1 + e^{-(Z)}}$) or the RELU function ($RELU(Z) = max(0, Z)$) are used (they're defined in the previous cell) ; \n",
    " - <b>Cost computation</b>: in which the cost function is calculated. <br>\n",
    "   As cost function the cross entropy is used: $J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))$;\n",
    " - <b>Backward pass</b>: in which the gradients for each layer are calculated. <br>\n",
    "   It start from the derivative of $Z$ which could be calculated applying: $dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$.\n",
    "   The, it calculates the other derivative needed in the next step, in particular: \n",
    "   $ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T}$, &nbsp;&nbsp;&nbsp;\n",
    "   $ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$ &nbsp;&nbsp; and &nbsp;&nbsp; $ dA^{[l-1]} = \\frac{\\partial \\mathcal{J} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]}$; <br>\n",
    "   In order to calculate the derivative of the activation function, $g'(Z^{[l]})$, this part of the code uses the sigmoid_backward and the relu_backward functions, which have been defined in the previous cell;\n",
    " - <b>Weights update</b>: in which the gradient descend is used to update the parameters $W$ and $b$. <br>\n",
    "   In particular for each node of the layer: $ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]}$ and $ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]}$, where $\\alpha$ is the selected learning rate.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:  \n",
    "    \"\"\"\n",
    "    This is the abstraction for a L-dimensional neural network used for a binary classification problem. \n",
    "    The net has L-1 hidden layers which exploit the relu activation, while the latest neuron exploits \n",
    "    the sigmoid activation function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layers_dims):\n",
    "        \"\"\"\n",
    "        Initializes the dimension and the parameter of the network.\n",
    "        \n",
    "        Parameters:\n",
    "        - layer_dims : list -> a list containg the dimension of each layer of the network, except the output layer since it is automatically added\n",
    "        \"\"\"\n",
    "        self.layers_dims = layers_dims + [1]\n",
    "        self.parameters = self.initialize_parameters_deep()\n",
    "        self.dimensions = len(self.layers_dims)-1\n",
    "        \n",
    "    \n",
    "    def initialize_parameters_deep(self):\n",
    "        \"\"\"\n",
    "        Initializes the weight and the bias for each neuron in each layer of the network. The weights initialization is made\n",
    "        with a random number divided by the square root of the layer's dimension. The biases are all initialized to 0.\n",
    "        \n",
    "        Returns:\n",
    "        parameters : python dictionary  -> containing the parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                        Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                        bl -- bias vector of shape (layer_dims[l], 1)\n",
    "        \"\"\"\n",
    "\n",
    "        np.random.seed(1) \n",
    "        parameters = {}\n",
    "        L = len(self.layers_dims)            # number of layers in the network\n",
    "\n",
    "        for l in range(1, L):\n",
    "            parameters['W' + str(l)] = np.random.randn(self.layers_dims[l], self.layers_dims[l-1]) / np.sqrt(self.layers_dims[l-1])\n",
    "            parameters['b' + str(l)] = np.zeros((self.layers_dims[l], 1))\n",
    "\n",
    "            assert(parameters['W' + str(l)].shape == (self.layers_dims[l], self.layers_dims[l-1]))\n",
    "            assert(parameters['b' + str(l)].shape == (self.layers_dims[l], 1))\n",
    "\n",
    "\n",
    "        return parameters\n",
    "    \n",
    "    def L_model_forward(self, X, parameters):\n",
    "        \"\"\"\n",
    "        Implements the forward propagation for the network. In particular it exploits the function \"linear_activation_forward\",\n",
    "        using the relu function for the first L-1 layers (where L is the number of layers). In the last layer the same function\n",
    "        is called, using the sigmoid function.\n",
    "\n",
    "        Arguments:\n",
    "        X : numpy array of shape (input size, number of examples) -> the dataset to learn or predict\n",
    "        parameters :  python dictionary -> output of initialize_parameters_deep() if the model is not trained, the model \n",
    "            parameters otherwise\n",
    "\n",
    "        Returns:\n",
    "        AL : numpy array -> the result of the sigmoid activation function for the last neuron\n",
    "        caches : list of caches -> every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1) \n",
    "            used in the backpropagation step to compute derivatives.\n",
    "        \"\"\"\n",
    "\n",
    "        caches = []\n",
    "        A = X\n",
    "        L = len(parameters) // 2                  # number of layers in the neural network\n",
    "\n",
    "        # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "        for l in range(1, L):\n",
    "            A_prev = A \n",
    "            A, cache = self.linear_activation_forward(A_prev, parameters['W' + str(l)],  parameters['b' + str(l)], \"relu\")\n",
    "            caches.append(cache)\n",
    "\n",
    "        # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "        AL, cache = self.linear_activation_forward(A, parameters['W' + str(L)],  parameters['b' + str(L)], \"sigmoid\")\n",
    "        caches.append(cache)\n",
    "\n",
    "        assert(AL.shape == (1,X.shape[1]))\n",
    "\n",
    "        return AL, caches\n",
    "    \n",
    "    def linear_activation_forward(self, A_prev, W, b, activation):\n",
    "        \"\"\"\n",
    "        This function is used by L-model_forward, first computes the linear step for a specific layer (exploiting the linear_forward \n",
    "        function, then it computes the non-linear activation (RELU or Sigmoid) function for that layer.\n",
    "\n",
    "        Arguments:\n",
    "        A_prev : numpy array (shape (size of previous layer, number of examples))-> activations from previous layer (or input data)\n",
    "        W : numpy array (shape (size of current layer, size of previous layer)) -> the weight matrix for the considered layer\n",
    "        b : numpy array (shape (size of the current layer, 1) -> the bias vector for the considered layer \n",
    "        activation : string -> the activation to be used in this layer, accepted value: \"sigmoid\" or \"relu\"\n",
    "\n",
    "        Returns:\n",
    "        A : numpy array (shape (size of considered layer, number of examples))-> the output of the activation function\n",
    "        cache :  python tuple -> it contains \"linear_cache\" and \"activation_cache\", stored for computing the backward pass efficiently\n",
    "        \"\"\"\n",
    "\n",
    "        if activation == \"sigmoid\":\n",
    "            # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "            Z, linear_cache = self.linear_forward(A_prev, W, b)\n",
    "            A, activation_cache = sigmoid(Z)\n",
    "\n",
    "        elif activation == \"relu\":\n",
    "            # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "            Z, linear_cache = self.linear_forward(A_prev, W, b)\n",
    "            A, activation_cache = relu(Z)\n",
    "\n",
    "        assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "        cache = (linear_cache, activation_cache)\n",
    "\n",
    "        return A, cache\n",
    "    \n",
    "    def linear_forward(self, A, W, b):\n",
    "        \"\"\"\n",
    "        This function is used by linear_activation_forward, it implements the linear part of a layer's forward propagation, \n",
    "        simply calculating WA+b (in a vectorized fashion)\n",
    "\n",
    "        Arguments:\n",
    "        A : numpy array (shape (size of previous layer, number of examples))-> activations from previous layer (or input data)\n",
    "        W : numpy array (shape (size of current layer, size of previous layer)) -> the weight matrix for the considered layer\n",
    "        b : numpy array (shape (size of the current layer, 1) -> the bias vector for the considered layer \n",
    "\n",
    "        Returns:\n",
    "        Z : numpy array (shape (size of current layer, number of examples)) -> the vector which will be the input of the activation function\n",
    "        cache : python tuple -> containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "        \"\"\"\n",
    "\n",
    "        Z = np.dot(W,A) + b\n",
    "\n",
    "        assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "        cache = (A, W, b)\n",
    "\n",
    "        return Z, cache\n",
    "    \n",
    "    def compute_cost(self, AL, Y):\n",
    "        \"\"\"\n",
    "        Implements the cross entropy cost function. \n",
    "\n",
    "        Arguments:\n",
    "        AL : numpy array (shape (1, number of examples)) -> the output of the last neuron, a probability vector corresponding to your label predictions\n",
    "        Y : numpy array (shape  (1, number of examples)) -> true \"label\" vector \n",
    "\n",
    "        Returns:\n",
    "        cost : numpy array -> it contains only a float: the cross-entropy cost\n",
    "        \"\"\"\n",
    "\n",
    "        m = Y.shape[1]\n",
    "\n",
    "        # Compute loss from aL and y.\n",
    "        cost = - 1/m * (np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T))\n",
    "\n",
    "        cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "        assert(cost.shape == ())\n",
    "\n",
    "        return cost\n",
    "    \n",
    "    def L_model_backward(self, AL, Y, caches):\n",
    "        \"\"\"\n",
    "        Implements the backward propagation for the network. In particular it first calculate the derivative for AL(the result\n",
    "        of the activation function on the last neuron). Then it exploits the function \"linear_activation_backward\", passing the \n",
    "        cached valued (voth linear and acrivation), to efficiently calculate the other required derivative. As always, \n",
    "        teh sigmoid derivative is calculated for the last neuron, while the relu derivative is apllied to the remaining layers.\n",
    "\n",
    "        Arguments:\n",
    "        AL : numpy array (shape (1, number of examples)) -> the output of the last neuron, a probability vector corresponding to your label predictions\n",
    "        Y : numpy array (shape  (1, number of examples)) -> true \"label\" vector \n",
    "        caches list of caches -> every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "\n",
    "        Returns:\n",
    "        grads : python dictionary -> It contains the gradients calculated. Example:\n",
    "                 grads[\"dA\" + str(l)] = ... \n",
    "                 grads[\"dW\" + str(l)] = ...\n",
    "                 grads[\"db\" + str(l)] = ... \n",
    "        \"\"\"\n",
    "        \n",
    "        grads = {}\n",
    "        L = len(caches) # the number of layers\n",
    "\n",
    "        # Initializing the backpropagation\n",
    "        dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
    "\n",
    "        # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "        current_cache = caches[L-1]\n",
    "        grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = self.linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "\n",
    "        # Loop from l=L-2 to l=0\n",
    "        for l in reversed(range(L-1)):\n",
    "            # lth layer: (RELU -> LINEAR) gradients.\n",
    "            current_cache = caches[l]\n",
    "            dA_prev_temp, dW_temp, db_temp = self.linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, \"relu\")\n",
    "            grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "            grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "            grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    def linear_activation_backward(self, dA, cache, activation):\n",
    "        \"\"\"\n",
    "        This function is used by L_model_backward, first computes the specific activation derivative, then it computes\n",
    "        the derviative for the linear part, exploiting \"linear_backward\" function.\n",
    "\n",
    "        Arguments:\n",
    "        dA : numpy array (shape (size of L-layer, number of examples))-> activation gradient for current layer l \n",
    "        cache :  python tuple -> it contains \"linear_cache\" and \"activation_cache\", used for computing the backward pass efficiently\n",
    "        activation : string -> the activation to be used in this layer, accepted value: \"sigmoid\" or \"relu\"\n",
    "\n",
    "        Returns:\n",
    "        dA_prev : numpy array (shape (size of (L-1)-layer, number of examples)) -> Gradient of the cost with respect to the activation (of the previous layer l-1)\n",
    "        dW : numpy array  -> Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "        db : numpy array -> Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "        \"\"\"\n",
    "        linear_cache, activation_cache = cache\n",
    "\n",
    "        if activation == \"relu\":\n",
    "            dZ = relu_backward(dA, activation_cache)\n",
    "            dA_prev, dW, db = self.linear_backward(dZ, linear_cache)\n",
    "\n",
    "        elif activation == \"sigmoid\":\n",
    "            dZ = sigmoid_backward(dA, activation_cache)\n",
    "            dA_prev, dW, db = self.linear_backward(dZ, linear_cache)\n",
    "\n",
    "        return dA_prev, dW, db\n",
    "    \n",
    "    def linear_backward(self, dZ, cache):\n",
    "        \"\"\"\n",
    "        Implements the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "        Arguments:\n",
    "        dZ : numpy array -> Gradient of the cost with respect to the linear output (of current layer l)\n",
    "        cache : tuple of values (A_prev, W, b) -> it comes from the forward propagation in the current layer\n",
    "\n",
    "        Returns:\n",
    "        dA_prev : numpy array -> Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "        dW : numpy array -> Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "        db : numoy array -> Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "        \"\"\"\n",
    "        A_prev, W, b = cache\n",
    "        m = A_prev.shape[1]\n",
    "\n",
    "        dW = 1/m * np.dot(dZ, A_prev.T)\n",
    "        db = 1/m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "        dA_prev = np.dot(W.T,dZ)\n",
    "\n",
    "        assert (dA_prev.shape == A_prev.shape)\n",
    "        assert (dW.shape == W.shape)\n",
    "        assert (db.shape == b.shape)\n",
    "\n",
    "        return dA_prev, dW, db\n",
    "    \n",
    "    def update_parameters(self, parameters, grads, learning_rate):\n",
    "        \"\"\"\n",
    "        Updates the parameters using gradient descent. \n",
    "\n",
    "        Arguments:\n",
    "        parameters : python dictionary -> the current model parameters (weights and biases for each neuron of each layer) \n",
    "        grads : python dictionary -> contains the calculated gradients, output of L_model_backward\n",
    "\n",
    "        Returns:\n",
    "        parameters : python dictionary -> the updated  model parameters\n",
    "        \"\"\"\n",
    "\n",
    "        L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "        # Update rule for each parameter\n",
    "        for l in range(L):\n",
    "            parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "            parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "        return parameters\n",
    "    \n",
    "    def fit(self, X, Y, learning_rate = 0.0075, epochs = 3000, print_cost=False, print_graph=False):\n",
    "        \"\"\"\n",
    "        The classic fit functions. It performs all the steps required to train the model: first the forward propagation,\n",
    "        later the cost computation, then the backward propagation and finally the weights updating via the classic\n",
    "        gradient descend.\n",
    "\n",
    "        Arguments:\n",
    "        X : numpy array  (shape (Number of features, number of examples)) -> data matrix in which each column represent a training example\n",
    "        Y : numoy array(shape (1, number of examples)) -> true \"label\" vector (each columns is the label of each example)\n",
    "        learning_rate : float -> learning rate of the gradient descent update rule\n",
    "        epochs : int -> number of iterations of the optimization loop\n",
    "        print_cost : boolean -> if True, it prints the cost every 100 steps\n",
    "        print_graph : boolean -> if True, it prints the cost graph at the end of the training process        \n",
    "        \n",
    "        Returns:\n",
    "        parameters : python dictionary -> the learned  model parameters\n",
    "        \"\"\"\n",
    "\n",
    "        costs = []                         # keep track of cost\n",
    "        \n",
    "        # Loop (gradient descent)\n",
    "        for i in range(0, epochs+1):\n",
    "\n",
    "            # Forward propagation\n",
    "            AL, caches = self.L_model_forward(X, self.parameters)\n",
    "\n",
    "            # Compute cost\n",
    "            cost = self.compute_cost(AL, Y)\n",
    "            if i % 100 == 0:\n",
    "                costs.append(cost)\n",
    "\n",
    "            # Backward propagation\n",
    "            grads = self.L_model_backward(AL, Y, caches)\n",
    "\n",
    "            # Update parameters\n",
    "            self.parameters = self.update_parameters(self.parameters, grads, learning_rate)\n",
    "\n",
    "            # Print the cost every 100 training example\n",
    "            if print_cost and i % 100 == 0:\n",
    "                print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "        # plot the cost\n",
    "        if print_graph:\n",
    "            plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "            plt.plot(np.squeeze(costs))\n",
    "            plt.ylabel('cost')\n",
    "            plt.xlabel('iterations (per hundreds)')\n",
    "            plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "            plt.show()\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        This function is used to predict the label of a the given examples. It exploits the weight calculated during the fit.\n",
    "\n",
    "        Arguments:\n",
    "         X : numpy array  (shape (Number of features, number of examples to predict)) -> data matrix in which each column represent an example to predict\n",
    "\n",
    "        Returns:\n",
    "        p : numpy array (shape (1, number of examples)) -> the predictions for the given dataset X\n",
    "        \"\"\"\n",
    "\n",
    "        m = X.shape[1]\n",
    "        #n = len(self.parameters) // 2 # number of layers in the neural network\n",
    "        p = np.zeros((1,m))\n",
    "\n",
    "        # Forward propagation\n",
    "        probas, caches = self.L_model_forward(X, self.parameters)\n",
    "\n",
    "\n",
    "        # convert probas to 0/1 predictions\n",
    "        for i in range(0, probas.shape[1]):\n",
    "            if probas[0,i] > 0.5:\n",
    "                p[0,i] = 1\n",
    "            else:\n",
    "                p[0,i] = 0\n",
    "\n",
    "        return p\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load of the catvnocat dataset, train and test fo the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y, classes = load_cat_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Neural_Network([12288, 20, 7, 5]) #model initialization\n",
    "model.fit(train_x, train_y, epochs = 2500, print_cost = True, print_graph = True) #model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Accuracy = \" + str(getAccuracy(model.predict(train_x),train_y)))\n",
    "print(\"Test Accuracy = \" + str(getAccuracy(model.predict(test_x),test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on custom images\n",
    "\n",
    "In order to test on custom image, you can add an image in the \"images\" folder. Then just set the \"my_image\" variable with the image name and run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imread\n",
    "from PIL import Image\n",
    "\n",
    "my_image = \"6.jpg\" # change this to the name of your image file \n",
    "\n",
    "fname = \"images/\" + my_image\n",
    "image = np.array(imread(fname))\n",
    "\n",
    "my_image = np.array(Image.fromarray(image).resize(size=(64,64))).reshape((64*64*3,1))\n",
    "my_image = my_image/255.\n",
    "\n",
    "my_predicted_image = model.predict(my_image)\n",
    "\n",
    "plt.imshow(image)\n",
    "print (\"y = \" + str(np.squeeze(my_predicted_image)) + \", your \" + str(model.dimensions) + \"-layer model predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indian dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y = load_indian_dataset(print_stats = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabet_model = Neural_Network([8, 6, 4, 2])\n",
    "diabet_model.fit(train_x_orig, train_y, epochs = 80000, print_cost = True, learning_rate = 0.0035, print_graph = True)\n",
    "print(\"Train Accuracy = \" + str(getAccuracy(diabet_model.predict(train_x_orig), train_y)))\n",
    "print(\"Test Accuracy = \" + str(getAccuracy(diabet_model.predict(test_x_orig), test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Learning rate too high?\n",
    "From the cost graph we could notice that the larning rate is too high so we can try to decrease it, increasing the \n",
    "number of epochs in order to get similar or better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabet_model = Neural_Network([8, 6, 4, 2])\n",
    "diabet_model.fit(train_x_orig, train_y, epochs = 140000, print_cost = True, learning_rate = 0.0015, print_graph = True)\n",
    "print(\"Train Accuracy = \" + str(getAccuracy(diabet_model.predict(train_x_orig), train_y)))\n",
    "print(\"Test Accuracy = \" + str(getAccuracy(diabet_model.predict(test_x_orig), test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A slight improvement\n",
    "Decreasing the learning rate and increasing the epoch number doesn't affect the accuracy on training set, but showed a slight improvement in test set accuracy. However better performance could be achieved in both the model (catvnoncat and indian-diabet) normalizing data and fine tuning the hyperparameter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
